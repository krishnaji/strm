
# -------------------http source agent and avro sink ------------------------------
source_agent.sources = apache_server
source_agent.sources.apache_server.type = http
source_agent.sources.apache_server.bind = 0.0.0.0
source_agent.sources.apache_server.port = 8000
source_agent.sources.apache_server.restart = true
source_agent.sources.apache_server.batchSize = 1
source_agent.sources.apache_server.channels = memoryChannel avroChannel

source_agent.channels = memoryChannel avroChannel
source_agent.channels.memoryChannel.type = memory
source_agent.channels.memoryChannel.keep-alive = 3
source_agent.channels.memoryChannel.capacity = 1000
source_agent.channels.avroChannel.type = memory
source_agent.channels.avroChannel.keep-alive = 3
source_agent.channels.avroChannel.capacity = 1000


source_agent.sinks = LocalOut avro_sink
source_agent.sinks.LocalOut.type = logger
source_agent.sinks.LocalOut.channel = memoryChannel

source_agent.sinks.avro_sink.type = avro
source_agent.sinks.avro_sink.channel = avroChannel
source_agent.sinks.avro_sink.hostname = 0.0.0.0
source_agent.sinks.avro_sink.port = 65000


# -------------------exec source agent------------------------------

source_agent.sources = apache_server
source_agent.sources.apache_server.type = exec
source_agent.sources.apache_server.command = uptime
source_agent.sources.apache_server.restart = true
source_agent.sources.apache_server.batchSize = 1
source_agent.sources.apache_server.channels = memoryChannel


source_agent.channels = memoryChannel
source_agent.channels.memoryChannel.type = memory
source_agent.channels.memoryChannel.keep-alive = 3
source_agent.channels.memoryChannel.capacity = 100


source_agent.sinks = avro_sink
source_agent.sinks.avro_sink.type = avro
source_agent.sinks.avro_sink.channel = memoryChannel
source_agent.sinks.avro_sink.hostname = 0.0.0.0
source_agent.sinks.avro_sink.port = 65000

# -------------------Collector hdfs------------------------------
collector.sources = AvroIn
collector.sources.AvroIn.type = avro
collector.sources.AvroIn.bind = 0.0.0.0
collector.sources.AvroIn.port = 65000
collector.sources.AvroIn.channels = mc1 mc2

collector.channels = mc1 mc2
collector.channels.mc1.type = memory
collector.channels.mc1.capacity = 100
collector.channels.mc2.type = memory
collector.channels.mc2.capacity = 1000000
collector.channels.mc2.maxFileSize=1623195647
collector.channels.mc2.transactionCapacity=1000
collector.sinks = LocalOut HadoopOut

collector.sinks.LocalOut.type = logger
collector.sinks.LocalOut.channel = mc1
collector.sinks.HadoopOut.type = hdfs
collector.sinks.HadoopOut.channel = mc2
collector.sinks.HadoopOut.hdfs.path = hdfs://fec0c959467a:8020/hdp
collector.sinks.HadoopOut.hdfs.fileType = DataStream
collector.sinks.HadoopOut.hdfs.writeFormat = Text
collector.sinks.HadoopOut.hdfs.serializer=avro_event
collector.sinks.HadoopOut.hdfs.serializer.compressionCodec=snappy
collector.sinks.HadoopOut.hdfs.fileSuffix=.avro
collector.sinks.HadoopOut.hdfs.rollSize = 10485760
collector.sinks.HadoopOut.hdfs.rollCount = 0
collector.sinks.HadoopOut.hdfs.rollInterval = 0


# -------------------Collector hbse------------------------------
collector.sources = AvroIn
collector.sources.AvroIn.type = avro
collector.sources.AvroIn.bind = 0.0.0.0
collector.sources.AvroIn.port = 65000
collector.sources.AvroIn.channels = mc1 mc2

collector.channels = mc1 mc2
collector.channels.mc1.type = memory
collector.channels.mc1.capacity = 100
collector.channels.mc2.type = memory
collector.channels.mc2.capacity = 1000000
collector.channels.mc2.maxFileSize=1623195647
collector.channels.mc2.transactionCapacity=1000
collector.sinks = LocalOut HadoopOut

collector.sinks.LocalOut.type = logger
collector.sinks.LocalOut.channel = mc1

collector.sinks.HadoopOut.type = asynchbase
collector.sinks.HadoopOut.channel = mc2
collector.sinks.HadoopOut.table = havc_event
collector.sinks.HadoopOut.columnFamily = events
collector.sinks.HadoopOut.serializer= org.apache.flume.sink.hbase.SimpleAsyncHbaseEventSerializer
collector.sinks.HadoopOut.serializer.payloadColumn = events
collector.sinks.HadoopOut.batchsize = 1000

#---------------

#flume-ng agent -c /etc/flume/conf/conf.empty -f /etc/flume/conf/conf.empty/flume2.conf -n collector -Dflume.root.logger=INFO,console
#flume-ng agent -c /etc/flume/conf/conf.empty -f /etc/flume/conf/conf.empty/flume.conf -n source_agent -Dflume.root.logger=INFO,console


import pandas as pd
import requests
import time
import json
url = 'http://104.236.211.22:8000'
df = pd.read_csv("/Users/Krishna/Downloads/SensorFiles/HVAC.csv",sep=',')
for i,row in df.iterrows():
	csv= (str(row['Date']),str(row['Time']),str(row['TargetTemp']),str(row['ActualTemp']),str(row['System']),str(row['BuildingID']))
	payload = [{
			"headers" : {
						"ID":i
						},
			"body": ",".join(csv)
			}
			]
	requests.post(url,data=str(payload))
	time.sleep(10)
	




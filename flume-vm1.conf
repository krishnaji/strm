
# -------------------http source agent and avro sink ------------------------------
source_agent.sources = apache_server
source_agent.sources.apache_server.type = http
source_agent.sources.apache_server.bind = 0.0.0.0
source_agent.sources.apache_server.port = 8000
source_agent.sources.apache_server.restart = true
source_agent.sources.apache_server.batchSize = 1
source_agent.sources.apache_server.channels = memoryChannel avroChannel

source_agent.channels = memoryChannel avroChannel
source_agent.channels.memoryChannel.type = memory
source_agent.channels.memoryChannel.keep-alive = 3
source_agent.channels.memoryChannel.capacity = 1000
source_agent.channels.avroChannel.type = memory
source_agent.channels.avroChannel.keep-alive = 3
source_agent.channels.avroChannel.capacity = 1000


source_agent.sinks = LocalOut avro_sink
source_agent.sinks.LocalOut.type = logger
source_agent.sinks.LocalOut.channel = memoryChannel

source_agent.sinks.avro_sink.type = avro
source_agent.sinks.avro_sink.channel = avroChannel
source_agent.sinks.avro_sink.hostname = 0.0.0.0
source_agent.sinks.avro_sink.port = 65000


# -------------------exec source agent------------------------------

source_agent.sources = apache_server
source_agent.sources.apache_server.type = exec
source_agent.sources.apache_server.command = uptime
source_agent.sources.apache_server.restart = true
source_agent.sources.apache_server.batchSize = 1
source_agent.sources.apache_server.channels = memoryChannel


source_agent.channels = memoryChannel
source_agent.channels.memoryChannel.type = memory
source_agent.channels.memoryChannel.keep-alive = 3
source_agent.channels.memoryChannel.capacity = 100


source_agent.sinks = avro_sink
source_agent.sinks.avro_sink.type = avro
source_agent.sinks.avro_sink.channel = memoryChannel
source_agent.sinks.avro_sink.hostname = 0.0.0.0
source_agent.sinks.avro_sink.port = 65000

# -------------------Collector hdfs------------------------------
collector.sources = AvroIn
collector.sources.AvroIn.type = avro
collector.sources.AvroIn.bind = 0.0.0.0
collector.sources.AvroIn.port = 65000
collector.sources.AvroIn.channels = mc1 mc2

collector.channels = mc1 mc2
collector.channels.mc1.type = memory
collector.channels.mc1.capacity = 100
collector.channels.mc2.type = memory
collector.channels.mc2.capacity = 1000000
collector.channels.mc2.maxFileSize=1623195647
collector.channels.mc2.transactionCapacity=1000
collector.sinks = LocalOut HadoopOut

collector.sinks.LocalOut.type = logger
collector.sinks.LocalOut.channel = mc1
collector.sinks.HadoopOut.type = hdfs
collector.sinks.HadoopOut.channel = mc2
collector.sinks.HadoopOut.hdfs.path = hdfs://fec0c959467a:8020/hdp
collector.sinks.HadoopOut.hdfs.fileType = DataStream
collector.sinks.HadoopOut.hdfs.writeFormat = Text
collector.sinks.HadoopOut.hdfs.serializer=avro_event
collector.sinks.HadoopOut.hdfs.serializer.compressionCodec=snappy
collector.sinks.HadoopOut.hdfs.fileSuffix=.avro
collector.sinks.HadoopOut.hdfs.rollSize = 10485760
collector.sinks.HadoopOut.hdfs.rollCount = 0
collector.sinks.HadoopOut.hdfs.rollInterval = 0


# -------------------Collector hbse------------------------------
collector.sources = AvroIn
collector.sources.AvroIn.type = avro
collector.sources.AvroIn.bind = 0.0.0.0
collector.sources.AvroIn.port = 65000
collector.sources.AvroIn.channels = mc1 mc2

collector.channels = mc1 mc2
collector.channels.mc1.type = memory
collector.channels.mc1.capacity = 100
collector.channels.mc2.type = memory
collector.channels.mc2.capacity = 1000000
collector.channels.mc2.maxFileSize=1623195647
collector.channels.mc2.transactionCapacity=1000
collector.sinks = LocalOut HadoopOut

collector.sinks.LocalOut.type = logger
collector.sinks.LocalOut.channel = mc1

collector.sinks.HadoopOut.type = asynchbase
collector.sinks.HadoopOut.channel = mc2
collector.sinks.HadoopOut.table = havc_event
collector.sinks.HadoopOut.columnFamily = events
collector.sinks.HadoopOut.serializer= org.apache.flume.sink.hbase.SimpleAsyncHbaseEventSerializer
collector.sinks.HadoopOut.serializer.payloadColumn = event
collector.sinks.HadoopOut.batchsize = 1000

#---------------

#flume-ng agent -c /etc/flume/conf/conf.empty -f /etc/flume/conf/conf.empty/flume2.conf -n collector -Dflume.root.logger=INFO,console
#flume-ng agent -c /etc/flume/conf/conf.empty -f /etc/flume/conf/conf.empty/flume.conf -n source_agent -Dflume.root.logger=INFO,console


import pandas as pd
import requests
import time
import json
url = 'http://104.236.211.22:8000'
df = pd.read_csv("/Users/Krishna/Downloads/SensorFiles/HVAC.csv",sep=',')
for i,row in df.iterrows():
	csv= (str(row['Date']),str(row['Time']),str(row['TargetTemp']),str(row['ActualTemp']),str(row['System']),str(row['BuildingID']))
	payload = [{
			"headers" : {
						"ID":i
						},
			"body": ",".join(csv)
			}
			]
	requests.post(url,data=str(payload))
	time.sleep(10)
	

#export HIVE_AUX_JARS_PATH=/usr/hdp/2.2.0.0-2041/hive-hcatalog/share/hcatalog/hive-hcatalog-core.
jar

#split.py
#!/usr/bin/python
import sys
def main(argv):
        csv = [argv]
        csvv = map(lambda x: x.split(','),csv)
        print csvv[0][0],csvv[0][1],csvv[0][2],csvv[0][3],csvv[0][4],csvv[0][5],csvv[0][6]
if __name__ == "__main__":
   main(sys.argv[1])

 # hive> CREATE EXTERNAL TABLE havc_event(key int, event string) 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = "events:events")
TBLPROPERTIES("hbase.table.name" = "havc_event");

 add FILE    split.py;
 
 [u'Date', u'Time', u'TargetTemp', u'ActualTemp', u'System', u'BuildingID'],


 select split(event,'[,]')[0] as Date,split(event,'[,]')[1] as Time,split(event,'[,]')[2] as TargetTemp,split(event,'[,]')[3] as ActualTemp,split(event,'[,]')[4] as System, split(event,'[,]')[5] as BuildingIDfrom havc_event;

